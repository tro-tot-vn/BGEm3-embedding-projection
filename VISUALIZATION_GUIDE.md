# ğŸ¨ Embedding Visualization Guide

**Last Updated:** October 26, 2025

---

## ğŸ“‹ Table of Contents

1. [Overview](#overview)
2. [Installation](#installation)
3. [Quick Start](#quick-start)
4. [Visualization Types](#visualization-types)
5. [Understanding the Plots](#understanding-the-plots)
6. [Command-Line Options](#command-line-options)
7. [Examples](#examples)
8. [Troubleshooting](#troubleshooting)

---

## ğŸ¯ Overview

After training your model, visualization helps you understand:

âœ… **What did the model learn?**
- Are queries and their positives close in embedding space?
- Are negatives pushed away?

âœ… **How well does it generalize?**
- Do similar features cluster together?
- Are locations separable?

âœ… **Where can it be improved?**
- Which queries have low similarity to their positives?
- Which negatives are too close to positives?

---

## ğŸ“¦ Installation

### Required Dependencies

```bash
# Core dependencies (already in requirements.txt)
pip install torch transformers sentence-transformers
pip install matplotlib seaborn numpy scikit-learn

# For UMAP visualization (optional but recommended)
pip install umap-learn
```

**Note:** UMAP is optional. If not installed, t-SNE will still work.

---

## ğŸš€ Quick Start

### Basic Usage

```bash
python visualize_embeddings.py \
    --checkpoint checkpoints/best_model.pt \
    --data data/gen-data-set.json \
    --output visualizations/
```

This will generate **5 types of plots** in the `visualizations/` directory:

1. `tsne_projection.png` - 2D t-SNE embedding space
2. `umap_projection.png` - 2D UMAP embedding space (if UMAP available)
3. `similarity_heatmap.png` - Query-document similarity matrix
4. `similarity_distribution.png` - Positive vs negative similarity histograms
5. `top_k_predictions.png` - Top-10 predictions for example queries

---

## ğŸ“Š Visualization Types

### 1. **t-SNE Projection** ğŸ“

**What it shows:**
- 2D projection of high-dimensional embeddings (256D â†’ 2D)
- Spatial relationships between queries and positives

**What to look for:**
- âœ… **Good:** Queries and their positives cluster together
- âŒ **Bad:** Queries and positives are scattered randomly

**Example interpretation:**

```
ğŸŸ¢ Query cluster â†’ Close to â†’ ğŸŸ¢ Positive cluster = Good!
ğŸ”µ Query cluster â†’ Far from â†’ ğŸ”´ Negative cluster = Great!
```

---

### 2. **UMAP Projection** ğŸ—ºï¸

**What it shows:**
- Similar to t-SNE but preserves more global structure
- Often clearer clustering than t-SNE

**Advantages over t-SNE:**
- Faster for large datasets
- Better at preserving both local and global structure
- More interpretable distances

**When to use:**
- Use UMAP for large datasets (>1000 samples)
- Use t-SNE for smaller datasets or when UMAP unavailable

---

### 3. **Similarity Heatmap** ğŸ”¥

**What it shows:**
- Matrix of cosine similarities: `sim[i, j] = cos(query_i, pos_j)`
- Diagonal = query-to-own-positive similarity (should be high!)
- Off-diagonal = query-to-other-positive similarity (should be lower)

**What to look for:**

```
Perfect heatmap:
     P1   P2   P3   P4
Q1  [ğŸŸ¢] [ğŸŸ¡] [ğŸŸ¡] [ğŸ”´]  â† Q1 matches P1 (diagonal = high)
Q2  [ğŸŸ¡] [ğŸŸ¢] [ğŸŸ¡] [ğŸ”´]  â† Q2 matches P2 (diagonal = high)
Q3  [ğŸŸ¡] [ğŸŸ¡] [ğŸŸ¢] [ğŸ”´]  â† Q3 matches P3 (diagonal = high)
Q4  [ğŸ”´] [ğŸ”´] [ğŸ”´] [ğŸŸ¢]  â† Q4 matches P4 (diagonal = high)

Legend: ğŸŸ¢ High sim (>0.8) | ğŸŸ¡ Medium (0.5-0.8) | ğŸ”´ Low (<0.5)
```

**Red flags:**
- âŒ Diagonal is not the brightest (positives not ranking #1)
- âŒ Many bright off-diagonal cells (false positives)

---

### 4. **Similarity Distribution** ğŸ“ˆ

**What it shows:**
- **Left plot:** Histogram of positive vs negative similarities
- **Right plot:** Box plot comparison

**What to look for:**

```
Good separation:
  Positive â”‚         [â”â”â”â”â”â”â”â”â”â”]      Mean: 0.85
           â”‚                              â†‘ High!
           â”‚ [â”â”â”â”â”â”]                  Mean: 0.45
  Negative â”‚                              â†‘ Low!
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
           0.0    0.5    1.0

Bad separation:
  Positive â”‚      [â”â”â”â”â”â”â”â”â”â”]         Overlap!
  Negative â”‚   [â”â”â”â”â”â”â”â”â”â”]            â† Both around 0.6
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

**Key metrics printed:**

```
ğŸ“Š Similarity Statistics:
   Positive: 0.8234 Â± 0.0876   â† Mean Â± std
   Negative: 0.4521 Â± 0.1234
   Margin:   0.3713             â† Difference (higher = better)
   Separation: 2.45Ïƒ            â† In units of std (>2 is good!)
```

**Interpretation:**
- âœ… **Margin > 0.3:** Good separation
- âœ… **Separation > 2Ïƒ:** Statistically significant
- âš ï¸ **Margin < 0.2:** Model may struggle to distinguish

---

### 5. **Top-K Predictions** ğŸ¯

**What it shows:**
- For each query, the top-10 most similar documents
- Green bars = correct positive match
- Red bars = incorrect matches

**What to look for:**

```
Good example:
Query: "TÃ¬m trá» Q10 25m2 5.5tr"
  Rank 1: [ğŸŸ¢â”â”â”â”â”â”â”â”â”â”â”â”â”â”] 0.92  â† Correct! (green)
  Rank 2: [ğŸ”´â”â”â”â”â”â”â”â”â”] 0.67       â† Wrong location
  Rank 3: [ğŸ”´â”â”â”â”â”â”â”] 0.55         â† Wrong price
  ...

Bad example:
Query: "TÃ¬m trá» Q10 25m2 5.5tr"
  Rank 1: [ğŸ”´â”â”â”â”â”â”â”â”â”] 0.68       â† Wrong! Should be green
  Rank 2: [ğŸ”´â”â”â”â”â”â”â”] 0.65
  Rank 3: [ğŸŸ¢â”â”â”â”â”â”] 0.62          â† Correct but rank 3 :(
  ...
```

**Good model:**
- Green bar is Rank 1 for most queries
- Large gap between Rank 1 and Rank 2

**Needs improvement:**
- Green bar is Rank 3+ (low MRR)
- Small gap between ranks (ambiguous predictions)

---

## âš™ï¸ Command-Line Options

### Full Options

```bash
python visualize_embeddings.py \
    --checkpoint PATH        # Required: Model checkpoint
    --data PATH              # Required: Dataset JSON
    --output DIR             # Output directory (default: visualizations/)
    --max-samples N          # Limit samples (default: 500)
    --batch-size N           # Encoding batch size (default: 32)
    --device cuda|cpu        # Device (default: auto-detect)
    --skip-umap              # Skip UMAP (faster)
    --skip-tsne              # Skip t-SNE (faster)
```

### Examples

**1. Quick visualization (100 samples):**

```bash
python visualize_embeddings.py \
    --checkpoint checkpoints/best_model.pt \
    --data data/gen-data-set.json \
    --max-samples 100 \
    --output quick_viz/
```

**2. Full dataset visualization:**

```bash
python visualize_embeddings.py \
    --checkpoint checkpoints/best_model.pt \
    --data data/gen-data-set.json \
    --max-samples 2000 \
    --output full_viz/
```

**3. Only t-SNE and heatmap (skip UMAP):**

```bash
python visualize_embeddings.py \
    --checkpoint checkpoints/best_model.pt \
    --data data/gen-data-set.json \
    --skip-umap
```

**4. CPU-only (no GPU):**

```bash
python visualize_embeddings.py \
    --checkpoint checkpoints/best_model.pt \
    --data data/gen-data-set.json \
    --device cpu
```

---

## ğŸ“š Examples

### Example 1: Training Progress

Compare checkpoints at different epochs:

```bash
# Early epoch
python visualize_embeddings.py \
    --checkpoint checkpoints/epoch_1.pt \
    --data data/gen-data-set.json \
    --output viz_epoch1/

# Mid training
python visualize_embeddings.py \
    --checkpoint checkpoints/epoch_5.pt \
    --data data/gen-data-set.json \
    --output viz_epoch5/

# Best model
python visualize_embeddings.py \
    --checkpoint checkpoints/best_model.pt \
    --data data/gen-data-set.json \
    --output viz_best/
```

Compare plots side-by-side to see improvement!

---

### Example 2: Dataset Quality Check

Visualize your dataset **before training**:

```bash
# Use untrained model (just projection head initialized)
python visualize_embeddings.py \
    --checkpoint model_init.pt \
    --data data/gen-data-set.json \
    --output viz_before_training/
```

**What to look for:**
- Even before training, frozen BGE-M3 should show some clustering
- If random â†’ dataset might have issues

---

### Example 3: Feature-Specific Analysis

Generate embeddings grouped by feature:

1. **By Location:**
   - Create separate datasets per district
   - Visualize each: Do Q10 queries cluster separately from Q1?

2. **By Price Range:**
   - Low (<4tr), Medium (4-7tr), High (>7tr)
   - Do they form distinct clusters?

3. **By Amenities:**
   - With AC vs without
   - Do they separate?

---

## ğŸ” Understanding the Plots

### What Good Embeddings Look Like

#### âœ… t-SNE/UMAP
```
      [Query Cluster]
            â†“
     (close together)
            â†“
    [Positive Cluster]

    (far from negatives)
```

#### âœ… Heatmap
```
Bright diagonal (>0.8)
Darker off-diagonal (<0.6)
```

#### âœ… Distribution
```
Positive mean: >0.75
Negative mean: <0.55
Margin: >0.20
```

#### âœ… Top-K
```
90%+ of queries have green bar at Rank 1
```

---

### What Bad Embeddings Look Like

#### âŒ t-SNE/UMAP
```
[Queries and Positives scattered randomly]
No clear clusters
```

#### âŒ Heatmap
```
No clear diagonal pattern
Many bright off-diagonal cells
```

#### âŒ Distribution
```
Positive and Negative overlap heavily
Margin < 0.15
```

#### âŒ Top-K
```
Many queries have green bar at Rank 3+
Small score gaps
```

---

## ğŸ› Troubleshooting

### Issue 1: UMAP Not Available

**Error:**
```
âš ï¸  UMAP not available. Install with: pip install umap-learn
```

**Fix:**
```bash
pip install umap-learn
```

Or skip UMAP:
```bash
python visualize_embeddings.py ... --skip-umap
```

---

### Issue 2: Out of Memory

**Error:**
```
RuntimeError: CUDA out of memory
```

**Fix 1:** Reduce batch size:
```bash
python visualize_embeddings.py ... --batch-size 8
```

**Fix 2:** Reduce samples:
```bash
python visualize_embeddings.py ... --max-samples 200
```

**Fix 3:** Use CPU:
```bash
python visualize_embeddings.py ... --device cpu
```

---

### Issue 3: t-SNE Too Slow

**Problem:** t-SNE takes forever on large datasets

**Fix 1:** Skip t-SNE, use UMAP only:
```bash
python visualize_embeddings.py ... --skip-tsne
```

**Fix 2:** Reduce samples:
```bash
python visualize_embeddings.py ... --max-samples 300
```

---

### Issue 4: Plots Look Weird

**Problem 1:** All points in one cluster
- **Cause:** Model not trained yet / embeddings not normalized
- **Fix:** Check model checkpoint is correct

**Problem 2:** No clear separation
- **Cause:** Model undertrained / dataset too hard
- **Fix:** Train longer or check dataset quality

**Problem 3:** Heatmap is all yellow
- **Cause:** All similarities around 0.5-0.7
- **Fix:** Model needs more training epochs

---

## ğŸ“Š Metrics Interpretation

### Similarity Margin

| Margin | Quality | Interpretation |
|--------|---------|----------------|
| > 0.40 | Excellent | Very strong separation |
| 0.30-0.40 | Good | Clear distinction between pos/neg |
| 0.20-0.30 | Fair | Usable but room for improvement |
| 0.10-0.20 | Poor | Model struggles to distinguish |
| < 0.10 | Bad | Model failed to learn |

---

### Separation (in Ïƒ)

| Separation | Quality | Interpretation |
|------------|---------|----------------|
| > 3.0Ïƒ | Excellent | Statistically very strong |
| 2.0-3.0Ïƒ | Good | Statistically significant |
| 1.5-2.0Ïƒ | Fair | Noticeable difference |
| 1.0-1.5Ïƒ | Poor | Weak separation |
| < 1.0Ïƒ | Bad | No real separation |

---

## ğŸ¯ Best Practices

### 1. Visualize Multiple Checkpoints
- Compare early vs late training
- Identify overfitting (if final checkpoint worse than earlier ones)

### 2. Sample Size Balance
- Too small (<100): Not representative
- Too large (>1000): t-SNE very slow
- **Recommended:** 300-500 samples

### 3. Interpret Together
- Don't rely on one plot
- Cross-reference: t-SNE + Distribution + Top-K

### 4. Dataset Quality Check
- Visualize BEFORE training (with random weights)
- If BGE-M3 base already shows good clustering â†’ good dataset
- If random even with base BGE-M3 â†’ check dataset

---

## ğŸ“ What to Report

When sharing results, include:

```markdown
## Embedding Visualization Results

### Model Info
- Checkpoint: checkpoints/epoch_10.pt
- Dataset: data/gen-data-set.json (500 samples)

### Key Metrics
- Positive similarity: 0.83 Â± 0.09
- Negative similarity: 0.42 Â± 0.12
- Margin: 0.41
- Separation: 3.1Ïƒ

### Visual Analysis
- t-SNE: Clear query-positive clustering âœ…
- Heatmap: Strong diagonal pattern âœ…
- Distribution: Good separation (minimal overlap) âœ…
- Top-K: 92% queries have correct positive at Rank 1 âœ…

### Conclusion
Model successfully learned to:
- âœ… Cluster similar queries and positives
- âœ… Separate negatives with clear margin
- âœ… Achieve high retrieval accuracy

[Attach plots]
```

---

## ğŸ“ Further Analysis Ideas

### 1. Error Analysis
- Find queries where green bar is NOT Rank 1
- Manually inspect: Why did model fail?
- Common patterns: specific feature types, locations, price ranges?

### 2. Feature Importance
- Compare similarities when only one feature differs
- Which features create largest distance?
- Does this match your weight-config.json?

### 3. Location Clustering
- Do all Q10 queries cluster together?
- Are neighboring districts (Q1, Q3, Q10) closer than distant ones?

### 4. Price Continuity
- Are 5tr and 5.5tr queries close?
- Is there a smooth gradient in price space?

---

## ğŸ”— Related Files

- **Main Training:** `train_script.py`, `TRAIN.md`
- **Evaluation:** `evaluate_model.py`, `EVALUATION_GUIDE.md`
- **Model:** `model.py`
- **Dataset:** `pair_dataset.py`, `data/gen-data-set.json`

---

## âœ… Quick Checklist

Before considering your model "done":

- [ ] Generated all 5 visualization types
- [ ] Positive mean similarity > 0.75
- [ ] Negative mean similarity < 0.55
- [ ] Margin > 0.25
- [ ] Separation > 2.0Ïƒ
- [ ] >85% queries have correct positive at Rank 1
- [ ] Clear clusters in t-SNE/UMAP
- [ ] Bright diagonal in heatmap

If all checked â†’ **Your model is ready for deployment!** ğŸš€

---

**Last Updated:** October 26, 2025  
**Part of:** BGE-M3 Fine-tuning for Vietnamese Rental Market

