# ğŸ¨ Visualization Tool - Implementation Summary

**Date:** October 26, 2025  
**Author:** BGE-M3 Fine-tuning Project  
**Purpose:** Visualize trained embedding space to understand model learning

---

## ğŸ“‹ Overview

The visualization tool provides **5 comprehensive visualizations** to help understand:
- âœ… What the model learned
- âœ… How well embeddings separate
- âœ… Which queries work well/poorly
- âœ… Whether to deploy or retrain

---

## ğŸ¯ Motivation

### **Why Visualization?**

After training and evaluation, you know:
- âœ… Training loss converged
- âœ… MRR and Recall metrics

But you DON'T know:
- â“ **HOW** does the model group similar items?
- â“ **WHERE** do failures occur?
- â“ **WHAT** patterns did it learn?

**Visualization answers these questions!**

---

## ğŸ“¦ Files Created

### 1. **`visualize_embeddings.py`** (554 lines)

**Purpose:** Main visualization script

**Features:**
- Loads trained model checkpoint
- Encodes queries and documents in batches
- Computes similarity matrices
- Generates 5 types of plots
- Outputs to `visualizations/` directory

**Key Classes:**
- `EmbeddingVisualizer`: Main class
  - `encode_batch()`: Batch encoding with progress bar
  - `compute_similarity_matrix()`: Cosine similarity
  - `plot_tsne()`: t-SNE 2D projection
  - `plot_umap()`: UMAP 2D projection
  - `plot_similarity_heatmap()`: Heatmap visualization
  - `plot_similarity_distribution()`: Histogram + boxplot
  - `plot_top_k_predictions()`: Ranked predictions

**Dependencies:**
```python
torch, numpy, matplotlib, seaborn
sklearn.manifold.TSNE
umap.UMAP (optional)
```

---

### 2. **`VISUALIZATION_GUIDE.md`** (611 lines)

**Purpose:** Comprehensive user guide

**Sections:**
1. Overview & Installation
2. Quick Start (1-command usage)
3. Visualization Types (detailed explanation of each)
4. Understanding Plots (good vs bad examples)
5. Command-Line Options
6. Examples (training progress, dataset quality)
7. Troubleshooting
8. Metrics Interpretation
9. Best Practices

---

### 3. **Updated `requirements.txt`**

**Added dependencies:**
```
# Visualization
matplotlib>=3.7.0,<3.10.0
seaborn>=0.12.0,<0.14.0

# Optional (for better visualizations)
# umap-learn>=0.5.0  # Uncomment for UMAP support
```

---

### 4. **Updated `00_START_HERE.md`**

**Changes:**
- Added "Visualize embeddings" to navigation table
- Added Step 3: Visualize (3 minutes) to Fastest Path
- Updated Complete Workflow to include visualization
- Added visualization commands to Quick Commands Reference
- Added "During Visualization" example output
- Added Visualization Success criteria
- Added visualization troubleshooting (UMAP, OOM)
- Updated version to 3.0

---

## ğŸ¨ Visualization Types

### **1. t-SNE Projection** ğŸ“

**What it shows:**
- 2D projection of 256-dimensional embeddings
- Spatial relationships between queries and positives

**Implementation:**
```python
from sklearn.manifold import TSNE

tsne = TSNE(n_components=2, perplexity=30, random_state=42)
coords_2d = tsne.fit_transform(embeddings)
```

**Good indicators:**
- âœ… Queries and their positives cluster together
- âœ… Clear separation between different clusters

**Bad indicators:**
- âŒ Random scatter (no clustering)
- âŒ All points in one cluster

---

### **2. UMAP Projection** ğŸ—ºï¸

**What it shows:**
- Alternative 2D projection (often better than t-SNE)
- Preserves both local and global structure

**Implementation:**
```python
from umap import UMAP

umap = UMAP(n_components=2, n_neighbors=15, random_state=42)
coords_2d = umap.fit_transform(embeddings)
```

**Advantages:**
- âš¡ Faster than t-SNE (important for >500 samples)
- ğŸ“ Better preserves global structure
- ğŸ¯ More consistent across runs

**Optional:** Automatically skipped if `umap-learn` not installed

---

### **3. Similarity Heatmap** ğŸ”¥

**What it shows:**
- Matrix: `sim[i, j] = cosine_similarity(query_i, positive_j)`
- Diagonal cells = query-to-own-positive (should be HIGH!)
- Off-diagonal = query-to-other-positive (should be LOWER)

**Implementation:**
```python
sim_matrix = query_embs @ pos_embs.T  # [N_q, N_d]

sns.heatmap(
    sim_matrix,
    cmap="RdYlGn",
    center=0.5,
    vmin=0.0,
    vmax=1.0
)
```

**Good indicators:**
- âœ… Bright diagonal (similarities > 0.8)
- âœ… Darker off-diagonal (similarities < 0.6)
- âœ… Clear pattern: diagonal >> others

**Bad indicators:**
- âŒ Dim diagonal (< 0.6)
- âŒ Bright off-diagonal (false positives)
- âŒ No clear pattern

---

### **4. Similarity Distribution** ğŸ“Š

**What it shows:**
- **Left:** Histogram of positive vs negative similarities
- **Right:** Box plot comparison

**Implementation:**
```python
# Extract similarities
pos_sim = [sim_matrix[i, i] for i in range(N)]
neg_sim = sim_matrix[~np.eye(N, dtype=bool)]

# Plot histogram
plt.hist(pos_sim, alpha=0.7, label='Positive', color='green')
plt.hist(neg_sim, alpha=0.7, label='Negative', color='red')

# Statistics
margin = pos_sim.mean() - neg_sim.mean()
separation = margin / (pos_sim.std() + neg_sim.std())
```

**Key Metrics:**
```
Positive: 0.8234 Â± 0.0876  â† Mean Â± std
Negative: 0.4521 Â± 0.1234
Margin:   0.3713            â† pos_mean - neg_mean
Separation: 2.45Ïƒ           â† margin / (pos_std + neg_std)
```

**Good indicators:**
- âœ… Margin > 0.25
- âœ… Separation > 2.0Ïƒ (statistically significant)
- âœ… Minimal overlap between distributions

**Bad indicators:**
- âŒ Margin < 0.15
- âŒ Separation < 1.5Ïƒ
- âŒ Heavy overlap

---

### **5. Top-K Predictions** ğŸ¯

**What it shows:**
- For each example query, show top-10 most similar documents
- Green bar = correct positive match
- Red bars = incorrect matches

**Implementation:**
```python
# For each query
for i in range(num_examples):
    # Get top-k predictions
    top_k_indices = np.argsort(sim_matrix[i])[::-1][:k]
    top_k_scores = sim_matrix[i, top_k_indices]
    
    # Color by correctness
    colors = ['green' if idx == ground_truth[i] else 'red' 
              for idx in top_k_indices]
    
    # Plot horizontal bar chart
    plt.barh(range(k), top_k_scores, color=colors)
```

**Good indicators:**
- âœ… Green bar at Rank 1 for most queries
- âœ… Large gap between Rank 1 and Rank 2
- âœ… All scores > 0.5

**Bad indicators:**
- âŒ Green bar at Rank 3+
- âŒ Small gaps between ranks
- âŒ Low absolute scores

---

## ğŸš€ Usage Examples

### **Example 1: Basic Visualization**

```bash
python visualize_embeddings.py \
    --checkpoint checkpoints/bgem3_projection_best.pt \
    --data data/gen-data-set.json
```

**Output:**
```
visualizations/
â”œâ”€â”€ tsne_projection.png
â”œâ”€â”€ umap_projection.png
â”œâ”€â”€ similarity_heatmap.png
â”œâ”€â”€ similarity_distribution.png
â””â”€â”€ top_k_predictions.png
```

---

### **Example 2: Quick Visualization (100 samples)**

```bash
python visualize_embeddings.py \
    --checkpoint checkpoints/bgem3_projection_best.pt \
    --data data/gen-data-set.json \
    --max-samples 100 \
    --output quick_viz/
```

**Time:** ~30 seconds

---

### **Example 3: Full Dataset (2000 samples)**

```bash
python visualize_embeddings.py \
    --checkpoint checkpoints/bgem3_projection_best.pt \
    --data data/gen-data-set.json \
    --max-samples 2000 \
    --output full_viz/
```

**Time:** ~3-5 minutes (t-SNE is slow)

---

### **Example 4: Skip UMAP (faster)**

```bash
python visualize_embeddings.py \
    --checkpoint checkpoints/bgem3_projection_best.pt \
    --data data/gen-data-set.json \
    --skip-umap
```

**Use when:** UMAP not installed or want faster execution

---

### **Example 5: Compare Training Progress**

```bash
# Epoch 1
python visualize_embeddings.py \
    --checkpoint checkpoints/bgem3_projection_epoch1.pt \
    --data data/gen-data-set.json \
    --output viz_epoch1/

# Epoch 5
python visualize_embeddings.py \
    --checkpoint checkpoints/bgem3_projection_epoch5.pt \
    --data data/gen-data-set.json \
    --output viz_epoch5/

# Best model
python visualize_embeddings.py \
    --checkpoint checkpoints/bgem3_projection_best.pt \
    --data data/gen-data-set.json \
    --output viz_best/

# Compare plots side-by-side!
```

---

## ğŸ“Š Interpretation Guidelines

### **What Good Embeddings Look Like**

| Visualization | Good Indicators |
|---------------|-----------------|
| **t-SNE/UMAP** | Clear clusters, queries close to positives |
| **Heatmap** | Bright diagonal (>0.8), dark off-diagonal (<0.6) |
| **Distribution** | Margin >0.25, Separation >2Ïƒ, minimal overlap |
| **Top-K** | >85% green at Rank 1, large gaps |

---

### **What Bad Embeddings Look Like**

| Visualization | Bad Indicators |
|---------------|----------------|
| **t-SNE/UMAP** | Random scatter, no clear clusters |
| **Heatmap** | Dim diagonal (<0.6), bright off-diagonal |
| **Distribution** | Margin <0.15, heavy overlap |
| **Top-K** | Green at Rank 3+, small gaps |

---

## ğŸ› Troubleshooting

### **Issue 1: UMAP Not Available**

```bash
pip install umap-learn
```

Or skip UMAP:
```bash
python visualize_embeddings.py ... --skip-umap
```

---

### **Issue 2: CUDA OOM**

```bash
# Option 1: Reduce batch size
python visualize_embeddings.py ... --batch-size 8

# Option 2: Reduce samples
python visualize_embeddings.py ... --max-samples 200

# Option 3: Use CPU
python visualize_embeddings.py ... --device cpu
```

---

### **Issue 3: t-SNE Too Slow**

```bash
# Skip t-SNE, use UMAP only
python visualize_embeddings.py ... --skip-tsne
```

---

## ğŸ“ Best Practices

### **1. Sample Size**

| Samples | Speed | Coverage | Recommendation |
|---------|-------|----------|----------------|
| 100 | âš¡ Fast (~30s) | Low | Quick check |
| 500 | ğŸš¶ Medium (~2m) | Good | âœ… **Recommended** |
| 2000 | ğŸŒ Slow (~5m) | High | Full analysis |

---

### **2. When to Visualize**

âœ… **Do visualize:**
- After training completes (understand what model learned)
- When metrics seem strange (debug issues)
- Before deployment (final validation)
- When comparing models (A/B testing)

âŒ **Don't visualize:**
- During training (use tensorboard for that)
- On every epoch (expensive)

---

### **3. Interpretation Workflow**

```
1. Check Distribution first
   â”œâ”€ Margin < 0.2? â†’ Model needs more training
   â””â”€ Margin > 0.25? â†’ Continue to step 2

2. Check t-SNE/UMAP
   â”œâ”€ Clear clusters? â†’ Good!
   â””â”€ Random scatter? â†’ Dataset/model issue

3. Check Heatmap
   â”œâ”€ Bright diagonal? â†’ Retrieval working
   â””â”€ Dim diagonal? â†’ Model not learning positives

4. Check Top-K
   â”œâ”€ Green at Rank 1? â†’ Deploy!
   â””â”€ Green at Rank 3+? â†’ Retrain or tune
```

---

## âœ… Success Checklist

Use this checklist to decide if your model is ready:

- [ ] **Distribution:** Margin > 0.25 âœ…
- [ ] **Distribution:** Separation > 2.0Ïƒ âœ…
- [ ] **t-SNE/UMAP:** Clear query-positive clustering âœ…
- [ ] **Heatmap:** Diagonal brightness > 0.8 âœ…
- [ ] **Heatmap:** Off-diagonal < 0.6 âœ…
- [ ] **Top-K:** >85% queries have green at Rank 1 âœ…
- [ ] **Top-K:** Gap between Rank 1 and Rank 2 > 0.1 âœ…

**If all checked â†’ Your model is production-ready! ğŸš€**

---

## ğŸ”— Integration with Workflow

### **Complete Training â†’ Evaluation â†’ Visualization Pipeline**

```bash
# Step 1: Train
python train_script.py --epochs 10

# Step 2: Evaluate
python evaluate_model.py

# Step 3: Visualize
python visualize_embeddings.py \
    --checkpoint checkpoints/bgem3_projection_best.pt \
    --data data/gen-data-set.json

# Step 4: Analyze
# - Open visualizations/*.png
# - Check success criteria
# - Decide: Deploy or Retrain
```

---

## ğŸ“ˆ Typical Results

### **Expected Output (Well-Trained Model)**

```
ğŸ“Š Similarity Statistics:
   Positive: 0.8234 Â± 0.0876  âœ… High mean
   Negative: 0.4521 Â± 0.1234  âœ… Low mean
   Margin:   0.3713           âœ… Large gap
   Separation: 2.45Ïƒ          âœ… Significant

Visualizations:
âœ“ tsne_projection.png       âœ… Clear clusters
âœ“ umap_projection.png       âœ… Good separation
âœ“ similarity_heatmap.png    âœ… Bright diagonal
âœ“ similarity_distribution.png âœ… Minimal overlap
âœ“ top_k_predictions.png     âœ… 92% correct at Rank 1
```

---

## ğŸ¯ Key Takeaways

1. **Visualization is essential** for understanding what your model learned
2. **5 complementary views** provide different insights:
   - t-SNE/UMAP: Global structure
   - Heatmap: Pairwise relationships
   - Distribution: Statistical separation
   - Top-K: Practical retrieval performance
3. **Quantitative metrics + Visual inspection = Complete picture**
4. **Use visualization to:**
   - Validate training success
   - Debug failures
   - Compare models
   - Build confidence before deployment

---

## ğŸ“š Related Documentation

- **User Guide:** `VISUALIZATION_GUIDE.md` (611 lines)
- **Training:** `TRAIN.md`
- **Evaluation:** `EVALUATION_GUIDE.md`
- **Getting Started:** `00_START_HERE.md`

---

## ğŸ‰ Conclusion

The visualization tool provides a **comprehensive, production-ready** solution for understanding trained embedding models.

**Key Features:**
- âœ… 5 types of visualizations
- âœ… Automatic statistics computation
- âœ… Batch processing for efficiency
- âœ… GPU/CPU support
- âœ… Extensive documentation
- âœ… Easy to use (1-command)

**Result:** You can now confidently answer:
- âœ… "Did my model learn correctly?"
- âœ… "Is it ready for deployment?"
- âœ… "Where are the failure modes?"

---

**Status:** âœ… Complete and Production-Ready  
**Date:** October 26, 2025  
**Version:** 1.0

